{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c479f6",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "---\n",
    "\n",
    "Using Pushshift's API, posts were gathered from the twitch and youtube subreddits. The code cells below are written as follows:\n",
    "\n",
    "1. Create the base URL, an empty dataframe, and the parameters to be used to get requests (subreddits and number of posts per (capped at 100 posts)\n",
    "2. To get 10,000 posts, while the size of the empty dataframe is less than 10,000:\n",
    "- Wait 5 seconds\n",
    "- Request 100 posts\n",
    "- Create a temporary dataframe from the response\n",
    "- Append the temporary dataframe to the main dataframe and re-index\n",
    "- Drop unwanted rows, nulls, media, and duplicates\n",
    "- Update parameters for the requests with the earliest retrieved post\n",
    "- Repeat\n",
    "3. Save the resulting dataframes\n",
    "\n",
    "**Please note**: Executing the retrieval cell blocks may cause lengthy run times due to wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3640a0f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78d09bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T05:12:44.959398Z",
     "start_time": "2021-11-05T05:12:43.921770Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import libaries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc40af",
   "metadata": {},
   "source": [
    "## Twitch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5159fdec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T21:58:55.688337Z",
     "start_time": "2021-11-03T21:58:55.673335Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create URL, parameters, and twitch_df\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "params = {\n",
    "    'subreddit':'twitch',\n",
    "    'size':100, #100 is the limit\n",
    "}\n",
    "\n",
    "twitch_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74526d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T22:13:52.227800Z",
     "start_time": "2021-11-03T21:58:55.689338Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# while-loop to get 10000 posts\n",
    "while twitch_df.shape[0] < 10000:\n",
    "    # wait 4 seconds before requesting\n",
    "    time.sleep(5)\n",
    "    res = requests.get(url,params)\n",
    "\n",
    "    # turning response into JSON, then grabbing 'data' and turning it into dataframe\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    posts_temp_df = pd.DataFrame(posts)\n",
    "    \n",
    "    # appending posts\n",
    "    twitch_df = twitch_df.append(posts_temp_df)\n",
    "    \n",
    "    # resetting index\n",
    "    twitch_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # removing where 'selftext' == '[removed]' | '[deleted]' | \"''\" or null\n",
    "    rmv_dlt_drop = twitch_df[(twitch_df['selftext'] == '[removed]') |\n",
    "                                (twitch_df['selftext'] == '[deleted]') |\n",
    "                                (twitch_df['selftext'] == '')]\n",
    "    twitch_df.drop(index = rmv_dlt_drop.index, inplace = True)\n",
    "    twitch_df.dropna(subset=['selftext'],inplace = True)\n",
    "\n",
    "    # removing where only media\n",
    "    media = twitch_df[twitch_df['media'].notnull()]\n",
    "    twitch_df.drop(index = media.index, inplace = True)\n",
    "\n",
    "    # removing duplicates\n",
    "    duplicates = twitch_df[twitch_df.duplicated(subset=['selftext']) == True]\n",
    "    twitch_df.drop(index = duplicates.index, inplace = True)\n",
    "    \n",
    "    # updating 'before' with 'created_utc' to get older posts\n",
    "    params.update({'before':posts[-1]['created_utc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadc65a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T22:13:52.840938Z",
     "start_time": "2021-11-03T22:13:52.229799Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# saving as CSVs\n",
    "twitch_df.to_csv('../data/twitch_df_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61964225",
   "metadata": {},
   "source": [
    "## Youtube data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7053290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T22:13:52.855940Z",
     "start_time": "2021-11-03T22:13:52.842938Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create URL, parameters, and youtube_df\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "params = {\n",
    "    'subreddit':'youtube',\n",
    "    'size':100, #100 is the limit\n",
    "}\n",
    "\n",
    "youtube_df = pd.DataFrame()\n",
    "eligible = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b581198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T22:48:05.506875Z",
     "start_time": "2021-11-03T22:13:52.857941Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# while-loop to get 10000 posts\n",
    "while youtube_df.shape[0] < 10000:\n",
    "    # wait 4 seconds before requesting\n",
    "    time.sleep(5)\n",
    "    res = requests.get(url,params)\n",
    "\n",
    "    # turning response into JSON, then grabbing 'data' and turning it into dataframe\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    posts_temp_df = pd.DataFrame(posts)\n",
    "\n",
    "    # appending posts\n",
    "    youtube_df = youtube_df.append(posts_temp_df)\n",
    "    \n",
    "    # resetting index\n",
    "    youtube_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # removing where only media\n",
    "    media = youtube_df[youtube_df['media'].notnull()]\n",
    "    youtube_df.drop(index = media.index, inplace = True)\n",
    "\n",
    "    # removing where 'selftext' == '[removed]' | '[deleted]' | \"''\" or null\n",
    "    rmv_dlt_drop = youtube_df[(youtube_df['selftext'] == '[removed]') |\n",
    "                                (youtube_df['selftext'] == '[deleted]') |\n",
    "                                (youtube_df['selftext'] == '')]\n",
    "    youtube_df.drop(index = rmv_dlt_drop.index, inplace = True)\n",
    "    youtube_df.dropna(subset=['selftext'],inplace = True)\n",
    "\n",
    "    # removing duplicates\n",
    "    duplicates = youtube_df[youtube_df.duplicated(subset = ['selftext']) == True]\n",
    "    youtube_df.drop(index = duplicates.index, inplace = True)\n",
    "    \n",
    "    # updating 'before' with 'created_utc' to get older posts\n",
    "    params.update({'before':posts[-1]['created_utc']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce291deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T22:48:05.900963Z",
     "start_time": "2021-11-03T22:48:05.507875Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# saving as CSVs\n",
    "youtube_df.to_csv('../data/youtube_df_full.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
